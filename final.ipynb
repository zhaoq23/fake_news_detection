{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is groups project for CAPP 30254: Machine Learning for Public Policy\n",
    "## Done by: Big Brother Debunkers ##\n",
    "**Team Member: Dingwei Liu, Qi Zhao**\n",
    "\n",
    "**Topic: Fake News Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZhaoQ\\AppData\\Local\\Temp\\ipykernel_50536\\1526268898.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd # data processing\n"
     ]
    }
   ],
   "source": [
    "# setting up the environment\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "import re  # regular expression\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Natural Language Toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.svm import SVC # Support Vector Classifier (SVC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It By Darrell Luc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the roundabout rather than heads in a straight line towar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, 2016 \\nThe tension between intelligence analysts a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Have Been Identified</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstrike Have Been Identified The rate at which civilia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished story about woman stoned to death for adultery</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to six years in prison after Iran’s Revolutionary Gu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                           title  \\\n",
       "0              House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It   \n",
       "1                                        FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart   \n",
       "2                                                              Why the Truth Might Get You Fired   \n",
       "3                                15 Civilians Killed In Single US Airstrike Have Been Identified   \n",
       "4  Iranian woman jailed for fictional unpublished story about woman stoned to death for adultery   \n",
       "\n",
       "               author  \\\n",
       "0       Darrell Lucus   \n",
       "1     Daniel J. Flynn   \n",
       "2  Consortiumnews.com   \n",
       "3     Jessica Purkiss   \n",
       "4      Howard Portnoy   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It By Darrell Luc...   \n",
       "1  Ever get the feeling your life circles the roundabout rather than heads in a straight line towar...   \n",
       "2  Why the Truth Might Get You Fired October 29, 2016 \\nThe tension between intelligence analysts a...   \n",
       "3  Videos 15 Civilians Killed In Single US Airstrike Have Been Identified The rate at which civilia...   \n",
       "4  Print \\nAn Iranian woman has been sentenced to six years in prison after Iran’s Revolutionary Gu...   \n",
       "\n",
       "   label  \n",
       "0      1  \n",
       "1      0  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv(\"train.csv\", encoding = \"UTF-8\")\n",
    "\n",
    "# check the first 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It\n",
      "1                                                FLYNN: Hillary Clinton, Big Woman on Campus - Breitbart\n",
      "2                                                                      Why the Truth Might Get You Fired\n",
      "3                                        15 Civilians Killed In Single US Airstrike Have Been Identified\n",
      "4          Iranian woman jailed for fictional unpublished story about woman stoned to death for adultery\n",
      "5    Jackie Mason: Hollywood Would Love Trump if He Bombed North Korea over Lack of Trans Bathrooms (...\n",
      "6    Life: Life Of Luxury: Elton John’s 6 Favorite Shark Pictures To Stare At During Long, Transconti...\n",
      "7                Benoît Hamon Wins French Socialist Party’s Presidential Nomination - The New York Times\n",
      "8    Excerpts From a Draft Script for Donald Trump’s Q&ampA With a Black Church’s Pastor - The New Yo...\n",
      "9          A Back-Channel Plan for Ukraine and Russia, Courtesy of Trump Associates - The New York Times\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# explore the title column\n",
    "print(data['title'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                 title  \\\n",
      "0                    House Dem Aide: We Didn’t Even See Comey’s Letter Until Jason Chaffetz Tweeted It   \n",
      "1                                                          FLYNN: Hillary Clinton, Big Woman on Campus   \n",
      "2                                                                    Why the Truth Might Get You Fired   \n",
      "3                                      15 Civilians Killed In Single US Airstrike Have Been Identified   \n",
      "4        Iranian woman jailed for fictional unpublished story about woman stoned to death for adultery   \n",
      "5  Jackie Mason: Hollywood Would Love Trump if He Bombed North Korea over Lack of Trans Bathrooms (...   \n",
      "6  Life: Life Of Luxury: Elton John’s 6 Favorite Shark Pictures To Stare At During Long, Transconti...   \n",
      "7                                   Benoît Hamon Wins French Socialist Party’s Presidential Nomination   \n",
      "8                  Excerpts From a Draft Script for Donald Trump’s Q&ampA With a Black Church’s Pastor   \n",
      "9                             A Back-Channel Plan for Ukraine and Russia, Courtesy of Trump Associates   \n",
      "\n",
      "               source  \n",
      "0                None  \n",
      "1           Breitbart  \n",
      "2                None  \n",
      "3                None  \n",
      "4                None  \n",
      "5           Breitbart  \n",
      "6                None  \n",
      "7  The New York Times  \n",
      "8  The New York Times  \n",
      "9  The New York Times  \n"
     ]
    }
   ],
   "source": [
    "# Split the 'title' column on the hyphen and expand to multiple columns\n",
    "split_data = data['title'].str.split(' - ', expand=True)\n",
    "\n",
    "# Assign the first part back to the 'title' column (if needed)\n",
    "data['title'] = split_data[0]\n",
    "\n",
    "# Create the new 'source' column from the second part of the split\n",
    "data['source'] = split_data[1]\n",
    "\n",
    "# Display the DataFrame to check the new 'source' column\n",
    "print(data[['title', 'source']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the dataset\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "title       558\n",
       "author     1957\n",
       "text         39\n",
       "label         0\n",
       "source    11812\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether there are missing values in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    10413\n",
       "0    10387\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of \"label\" in the dataset\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 authors:\n",
      "author\n",
      "Pam Key             243\n",
      "admin               193\n",
      "Jerome Hudson       166\n",
      "Charlie Spiering    141\n",
      "John Hayward        140\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 5 sources:\n",
      "source\n",
      "The New York Times    6222\n",
      "Breitbart             2254\n",
      "The Onion               76\n",
      "Russia News Now         50\n",
      "RT Arabic               15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the top 5 authors\n",
    "top_authors = data['author'].value_counts().head(5)\n",
    "\n",
    "# Get the top 5 sources\n",
    "top_sources = data['source'].value_counts().head(5)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 authors:\")\n",
    "print(top_authors)\n",
    "print(\"\\nTop 5 sources:\")\n",
    "print(top_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution for missing 'author':\n",
      "label\n",
      "1    1931\n",
      "0      26\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution for non-missing 'author':\n",
      "label\n",
      "0    10361\n",
      "1     8482\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution for missing 'source':\n",
      "label\n",
      "1    10004\n",
      "0     1808\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label distribution for non-missing 'source':\n",
      "label\n",
      "0    8579\n",
      "1     409\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Masks for missing and non-missing values in 'author'\n",
    "missing_author = data['author'].isnull()\n",
    "not_missing_author = data['author'].notnull()\n",
    "\n",
    "# Masks for missing and non-missing values in 'source'\n",
    "missing_source = data['source'].isnull()\n",
    "not_missing_source = data['source'].notnull()\n",
    "\n",
    "# Distribution of 'label' for missing and non-missing 'author'\n",
    "author_missing_label_dist = data.loc[missing_author, 'label'].value_counts()\n",
    "author_not_missing_label_dist = data.loc[not_missing_author, 'label'].value_counts()\n",
    "\n",
    "# Distribution of 'label' for missing and non-missing 'source'\n",
    "source_missing_label_dist = data.loc[missing_source, 'label'].value_counts()\n",
    "source_not_missing_label_dist = data.loc[not_missing_source, 'label'].value_counts()\n",
    "\n",
    "# Print the results\n",
    "print(\"Label distribution for missing 'author':\")\n",
    "print(author_missing_label_dist)\n",
    "print(\"\\nLabel distribution for non-missing 'author':\")\n",
    "print(author_not_missing_label_dist)\n",
    "\n",
    "print(\"\\nLabel distribution for missing 'source':\")\n",
    "print(source_missing_label_dist)\n",
    "print(\"\\nLabel distribution for non-missing 'source':\")\n",
    "print(source_not_missing_label_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the missing values in text column\n",
    "data = data.dropna(subset=['title'])\n",
    "\n",
    "# fill in missing values with empty strings\n",
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the missing values in text column\n",
    "data = data.dropna(subset=['title'])\n",
    "\n",
    "# fill in missing values with empty strings\n",
    "data = data.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the title, author name, and text columns\n",
    "data['content'] = data['title'] + ' ' + data['author'] + ' ' + data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem = PorterStemmer() \n",
    "\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower() \n",
    "    stemmed_content = stemmed_content.split() \n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] \n",
    "    stemmed_content = ' '.join(stemmed_content) \n",
    "    return stemmed_content\n",
    "\n",
    "data['content'] = data['content'].apply(stemming)\n",
    "\n",
    "data['content'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word  Frequency\n",
      "23    the     814841\n",
      "42     to     421828\n",
      "108    of     416548\n",
      "117   and     363386\n",
      "36      a     344519\n",
      "25     in     281279\n",
      "75   that     195537\n",
      "46     is     150257\n",
      "205   for     137180\n",
      "18     on     122625\n",
      "84    was      99669\n",
      "145  with      97789\n",
      "52    The      89583\n",
      "71     as      89208\n",
      "271    he      74646\n",
      "621    by      74019\n",
      "217  have      70727\n",
      "622   are      70513\n",
      "63     it      70100\n",
      "89     be      69140\n"
     ]
    }
   ],
   "source": [
    "words = [word for row in data['content'] for word in row.split()]\n",
    "\n",
    "# count the frequency of each word\n",
    "word_counts = Counter(words)\n",
    "\n",
    "df_word_counts = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "df_word_counts = df_word_counts.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "print(df_word_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a word cloud object\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m \u001b[43mWordCloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Display the word cloud using matplotlib\u001b[39;00m\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\wordcloud\\wordcloud.py:642\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\wordcloud\\wordcloud.py:623\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(words)\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\wordcloud\\wordcloud.py:598\u001b[0m, in \u001b[0;36mWordCloud.process_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    596\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([i\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords])\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollocations:\n\u001b[1;32m--> 598\u001b[0m     word_counts \u001b[38;5;241m=\u001b[39m \u001b[43munigrams_and_bigrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_plurals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollocation_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# remove stopwords\u001b[39;00m\n\u001b[0;32m    601\u001b[0m     words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\wordcloud\\tokenization.py:44\u001b[0m, in \u001b[0;36munigrams_and_bigrams\u001b[1;34m(words, stopwords, normalize_plurals, collocation_threshold)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munigrams_and_bigrams\u001b[39m(words, stopwords, normalize_plurals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collocation_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# We must create the bigrams before removing the stopword tokens from the words, or else we get bigrams like\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# \"thank much\" from \"thank you very much\".\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# We don't allow any of the words in the bigram to be stopwords\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pairwise(words) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stopwords \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m p))\n\u001b[1;32m---> 44\u001b[0m     unigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords)\n\u001b[0;32m     45\u001b[0m     n_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unigrams)\n\u001b[0;32m     46\u001b[0m     counts_unigrams, standard_form \u001b[38;5;241m=\u001b[39m process_tokens(\n\u001b[0;32m     47\u001b[0m         unigrams, normalize_plurals\u001b[38;5;241m=\u001b[39mnormalize_plurals)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\wordcloud\\tokenization.py:44\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munigrams_and_bigrams\u001b[39m(words, stopwords, normalize_plurals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collocation_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# We must create the bigrams before removing the stopword tokens from the words, or else we get bigrams like\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# \"thank much\" from \"thank you very much\".\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# We don't allow any of the words in the bigram to be stopwords\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     bigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pairwise(words) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stopwords \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m p))\n\u001b[1;32m---> 44\u001b[0m     unigrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords)\n\u001b[0;32m     45\u001b[0m     n_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unigrams)\n\u001b[0;32m     46\u001b[0m     counts_unigrams, standard_form \u001b[38;5;241m=\u001b[39m process_tokens(\n\u001b[0;32m     47\u001b[0m         unigrams, normalize_plurals\u001b[38;5;241m=\u001b[39mnormalize_plurals)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Combine all texts into one large string\n",
    "text = ' '.join(data['content'])\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the word cloud using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing (Textual data to numerical data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "1    FLYNN: Hillary Clinton, Big Woman on Campus  D...\n",
       "2    Why the Truth Might Get You Fired Consortiumne...\n",
       "3    15 Civilians Killed In Single US Airstrike Hav...\n",
       "4    Iranian woman jailed for fictional unpublished...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['content'] = data['content'].values\n",
    "data['label'] = data['label'].values\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data['content'])\n",
    "\n",
    "X = vectorizer.transform(data['content'])\n",
    "data['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y values\n",
    "X = X\n",
    "y = data['label']\n",
    "\n",
    "# 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=42)\n",
    "\n",
    "# 20% testing and 10% validation\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=1/3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5404)\t0.06715358082131856\n",
      "  (0, 6617)\t0.026078384500183523\n",
      "  (0, 6662)\t0.03307861290125485\n",
      "  (0, 8102)\t0.024829178667655646\n",
      "  (0, 9315)\t0.02091729748400212\n",
      "  (0, 9504)\t0.10275763950401619\n",
      "  (0, 12422)\t0.039094912504131434\n",
      "  (0, 12867)\t0.1156161514810952\n",
      "  (0, 14910)\t0.034457321869708755\n",
      "  (0, 17020)\t0.040698598120079914\n",
      "  (0, 17119)\t0.08384045298875266\n",
      "  (0, 17341)\t0.023907070392065057\n",
      "  (0, 17385)\t0.03142931184393018\n",
      "  (0, 17416)\t0.047427455411629585\n",
      "  (0, 18755)\t0.03692701196536229\n",
      "  (0, 22372)\t0.06261472979482217\n",
      "  (0, 24399)\t0.019640327857536128\n",
      "  (0, 25238)\t0.0388615282984522\n",
      "  (0, 26391)\t0.0427742331942849\n",
      "  (0, 28590)\t0.04734424228897373\n",
      "  (0, 30335)\t0.11613571239636801\n",
      "  (0, 31605)\t0.06866039963868291\n",
      "  (0, 32214)\t0.09164732091506263\n",
      "  (0, 37598)\t0.10010791715277705\n",
      "  (0, 39473)\t0.06614482915747129\n",
      "  :\t:\n",
      "  (13493, 147336)\t0.03119479006199519\n",
      "  (13493, 147571)\t0.05321207241517188\n",
      "  (13493, 147979)\t0.014396736873992563\n",
      "  (13493, 148269)\t0.03719644810959659\n",
      "  (13493, 148379)\t0.052169481482172\n",
      "  (13493, 151655)\t0.020067515152973395\n",
      "  (13493, 153653)\t0.03582823887139408\n",
      "  (13493, 154287)\t0.04791703019035358\n",
      "  (13493, 154288)\t0.05724831729352596\n",
      "  (13493, 154358)\t0.013411073173423506\n",
      "  (13493, 156682)\t0.030174213460776173\n",
      "  (13493, 157406)\t0.04693805398326054\n",
      "  (13493, 159063)\t0.042441013761212475\n",
      "  (13493, 159156)\t0.027533253603992017\n",
      "  (13493, 159451)\t0.03573703773695896\n",
      "  (13493, 159609)\t0.03129356670515384\n",
      "  (13493, 160332)\t0.024983261974157213\n",
      "  (13493, 160927)\t0.03817298294109442\n",
      "  (13493, 161403)\t0.029268984229103845\n",
      "  (13493, 162070)\t0.017502943723918817\n",
      "  (13493, 162175)\t0.01314819398070527\n",
      "  (13493, 162664)\t0.08448847142867164\n",
      "  (13493, 162686)\t0.45820293409803636\n",
      "  (13493, 163292)\t0.016554472343586632\n",
      "  (13493, 163760)\t0.019226389110137434\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# classifier.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m, y_train\u001b[38;5;241m.\u001b[39mvalues)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# classifier.fit(X_train, y_train)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9657625611382836\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, X_predict)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 0.9648882808937529\n"
     ]
    }
   ],
   "source": [
    "# calculate f1-score\n",
    "f1 = f1_score(y_test, X_predict)\n",
    "\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model =  LogisticRegression()  # 建模\n",
    "model.fit(X_train,  y_train)  # 拟合\n",
    "y_pred =  model.predict(X_test)  # 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9486438417074256"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,  y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
